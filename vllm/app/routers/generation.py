from fastapi import APIRouter, HTTPException
from vllm import SamplingParams
from vllm.lora.request import LoRARequest
import uuid
import logging

from app.models import GenerateRequest, GenerateResponse
from app import core
from app.utils.prompt_utils import create_chat_prompt
from app.utils.response_utils import clean_response

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/generate", response_model=GenerateResponse)
async def generate_response_endpoint(request: GenerateRequest):
    """ì¸í”Œë£¨ì–¸ì„œ ì‘ë‹µ ìƒì„±"""
    logger.info(f"ğŸ”„ ì‘ë‹µ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨")
    logger.info(f"ğŸ“‹ ìš”ì²­ ë°ì´í„°: {request.dict()}")
    
    logger.info(f"ğŸ” í˜„ì¬ ì—”ì§„ ìƒíƒœ í™•ì¸: {core.engine}")
    if core.engine is None:
        logger.error("âŒ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        raise HTTPException(status_code=500, detail="ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    logger.info(f"âœ… ì—”ì§„ ìƒíƒœ í™•ì¸ ì™„ë£Œ: {type(core.engine)}")
    
    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¬´ì¡°ê±´ chat template ì‚¬ìš©)
        formatted_prompt = create_chat_prompt(
            request.user_message, 
            request.system_message, 
            request.influencer_name
        )
        
        logger.info(f"ğŸ” ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 200ì): {formatted_prompt[:200]}...")
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            temperature=request.temperature,
            max_tokens=request.max_new_tokens,
            top_p=0.9,
            top_k=50,
            stop=["[|Human|", "[|System|", "<|im_end|", "</s>", "<|eot_id|>"],
            repetition_penalty=1.1
        )
        
        # LoRA ìš”ì²­ ì„¤ì •
        lora_request = None
        used_adapter = False
        
        if request.model_id:
            if request.model_id not in core.loaded_adapters:
                raise HTTPException(
                    status_code=400,
                    detail=f"ì–´ëŒ‘í„° {request.model_id}ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € /load_adapterë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                )
            
            adapter_info = core.loaded_adapters[request.model_id]
            lora_request = LoRARequest(
                lora_name=request.model_id,
                lora_int_id=adapter_info["lora_int_id"],
                lora_path=adapter_info["hf_repo_name"]
            )
            used_adapter = True
            logger.info(f"ğŸ”§ LoRA ì–´ëŒ‘í„° ì‚¬ìš©: {request.model_id}")
        
        # ê³ ìœ  request_id ìƒì„±
        request_id = str(uuid.uuid4())
        
        # ë¹„ë™ê¸° ìƒì„±
        results = []
        async for output in core.engine.generate(
            formatted_prompt,
            sampling_params,
            request_id=request_id,
            lora_request=lora_request
        ):
            results.append(output)
        
        if not results:
            raise HTTPException(status_code=500, detail="ìƒì„±ëœ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‘ë‹µ ì²˜ë¦¬
        final_output = results[-1]
        raw_response = final_output.outputs[0].text
        
        # ì‘ë‹µ ì •ë¦¬
        cleaned_response = clean_response(raw_response, request.influencer_name)
        
        logger.info(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {request.influencer_name}")
        
        return GenerateResponse(
            response=cleaned_response,
            model_id=request.model_id,
            used_adapter=used_adapter,
            formatted_prompt=formatted_prompt,
            raw_response=raw_response
        )
        
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        logger.error(f"âŒ ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
        import traceback
        logger.error(f"âŒ ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")