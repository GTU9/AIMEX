"""
OpenAI API í´ë¼ì´ì–¸íŠ¸ ë˜í¼

ì´ ëª¨ë“ˆì€ í”„ë¡œì íŠ¸ ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” OpenAI API í˜¸ì¶œì„ í‘œì¤€í™”í•©ë‹ˆë‹¤.
- ê³µí†µ ì—ëŸ¬ ì²˜ë¦¬
- ì¬ì‹œë„ ë¡œì§
- ë¡œê¹…
- ì„¤ì • ê´€ë¦¬
"""

import os
import logging
import asyncio
from typing import List, Dict, Any, Optional, Union
from openai import AsyncOpenAI
import random

logger = logging.getLogger(__name__)


class OpenAIClientWrapper:
    """OpenAI API í´ë¼ì´ì–¸íŠ¸ ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: Optional[str] = None, timeout: int = 60, max_retries: int = 3):
        """
        OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)
            timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.timeout = timeout
        self.max_retries = max_retries
        
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        self.client = AsyncOpenAI(api_key=self.api_key, timeout=timeout)
        logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        ì±„íŒ… ì™„ì„± API í˜¸ì¶œ
        
        Args:
            messages: ë©”ì‹œì§€ ëª©ë¡
            model: ì‚¬ìš©í•  ëª¨ë¸
            temperature: ì˜¨ë„ íŒŒë¼ë¯¸í„°
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Raises:
            Exception: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        for attempt in range(self.max_retries + 1):
            try:
                logger.debug(f"OpenAI API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{self.max_retries + 1}")
                
                response = await self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs
                )
                
                result = response.choices[0].message.content.strip()
                logger.debug(f"âœ… OpenAI API í˜¸ì¶œ ì„±ê³µ (ê¸¸ì´: {len(result)} ë¬¸ì)")
                return result
                
            except Exception as e:
                logger.warning(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                
                if attempt < self.max_retries:
                    # ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ë¹„ë™ê¸° ì¬ì‹œë„
                    delay = (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"â³ {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"ğŸš« OpenAI API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    raise
    
    async def generate_question(
        self,
        character_info: str,
        temperature: float = 0.6,
        model: str = "gpt-4o-mini"
    ) -> str:
        """
        ìºë¦­í„° ì •ë³´ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        
        Args:
            character_info: ìºë¦­í„° ì •ë³´
            temperature: ì˜¨ë„ íŒŒë¼ë¯¸í„°
            model: ì‚¬ìš©í•  ëª¨ë¸
            
        Returns:
            str: ìƒì„±ëœ ì§ˆë¬¸
        """
        prompt = f"""
ë‹¹ì‹ ì€ ì•„ë˜ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ ìºë¦­í„°ê°€ ê°€ì¥ ì˜ ë“œëŸ¬ë‚  ìˆ˜ ìˆëŠ” ìƒí™©ì´ë‚˜ ì¼ìƒì ì¸ ì§ˆë¬¸ í•˜ë‚˜ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ìºë¦­í„° ì •ë³´]
{character_info}

ì¡°ê±´:
- ì§ˆë¬¸ì€ ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ì§ˆë¬¸ì€ ì¼ìƒì ì¸ ëŒ€í™”ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ì§ˆë¬¸ì˜ ë§íˆ¬ë‚˜ ë‹¨ì–´ ì„ íƒë„ ìºë¦­í„°ê°€ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ ìœ ë„í•´ì£¼ì„¸ìš”.
"""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ìºë¦­í„° ê¸°ë°˜ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        return await self.chat_completion(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=100
        )
    
    async def summarize_speech_style(
        self,
        system_prompt: str,
        model: str = "gpt-4o-mini"
    ) -> Dict[str, str]:
        """
        ë§íˆ¬ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì§•ì„ ìš”ì•½
        
        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            model: ì‚¬ìš©í•  ëª¨ë¸
            
        Returns:
            Dict[str, str]: í•´ì‹œíƒœê·¸ì™€ ì„¤ëª…ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        system_instruction = """
ì£¼ì–´ì§„ ë§íˆ¬ì˜ system promptë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ ë§íˆ¬ì˜ íŠ¹ì§•ì„ ìš”ì•½í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ì§€ì¼œì„œ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

í˜•ì‹:
{
    "hashtags": "#í‚¤ì›Œë“œ1 #í‚¤ì›Œë“œ2 #í‚¤ì›Œë“œ3",
    "description": "ë§íˆ¬ ì„¤ëª… (í•œ ë¬¸ì¥, '~ë§íˆ¬'ë¡œ ëë‚˜ì•¼ í•¨)"
}

ì¡°ê±´:
1. ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ MZ ëŠë‚Œë‚˜ê²Œ í‚¤ì›Œë“œ 3ê°œë¥¼ ìƒì„±í•´ í•´ì‹œíƒœê·¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
2. ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ 'ë§íˆ¬'ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤. ì„œìˆ ì–´ ì—†ì´ ëª…ì‚¬í˜•ìœ¼ë¡œ ëë‚©ë‹ˆë‹¤.
3. ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”. (ì¶”ê°€ ì„¤ëª… ì—†ì´)
"""

        messages = [
            {"role": "system", "content": system_instruction},
            {"role": "user", "content": f"ë§íˆ¬ ì§€ì‹œì‚¬í•­:\n{system_prompt}"}
        ]
        
        try:
            response = await self.chat_completion(
                messages=messages,
                model=model,
                temperature=0.7,
                max_tokens=200
            )
            
            import json
            return json.loads(response)
            
        except Exception as e:
            logger.error(f"ë§íˆ¬ ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "hashtags": "#GPT #ì‘ë‹µíŒŒì‹± #ì‹¤íŒ¨",
                "description": "ë§íˆ¬ ìš”ì•½ ì‹¤íŒ¨í•œ ë§íˆ¬"
            }
    
    async def generate_system_prompt_for_tone(
        self,
        character_info: str,
        tone_variation: int,
        model: str = "gpt-4o-mini"
    ) -> str:
        """
        ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ë§íˆ¬ì— ëŒ€í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            character_info: ìºë¦­í„° ì •ë³´
            tone_variation: ë§íˆ¬ ë³€í˜• ë²ˆí˜¸ (1, 2, 3)
            model: ì‚¬ìš©í•  ëª¨ë¸
            
        Returns:
            str: ìƒì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        tone_instructions = {
            1: "ì£¼ì–´ì§„ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²« ë²ˆì§¸ ë…íŠ¹í•˜ê³  ì°½ì˜ì ì¸ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”. ìºë¦­í„°ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ë˜ ì˜ˆìƒì¹˜ ëª»í•œ ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.",
            2: "ì£¼ì–´ì§„ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‘ ë²ˆì§¸ ë…íŠ¹í•˜ê³  ì°½ì˜ì ì¸ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì²« ë²ˆì§¸ì™€ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.",
            3: "ì£¼ì–´ì§„ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¸ ë²ˆì§¸ ë…íŠ¹í•˜ê³  ì°½ì˜ì ì¸ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì•ì˜ ë‘ ê°€ì§€ì™€ëŠ” ì „í˜€ ë‹¤ë¥¸ ì°¸ì‹ í•œ ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”."
        }
        
        tone_instruction = tone_instructions.get(tone_variation, "ìºë¦­í„°ì˜ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•œ ì°½ì˜ì  ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        prompt = f"""
[ìš”ì²­ ì¡°ê±´]
ë‹¤ìŒ ìºë¦­í„° ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ GPTì˜ ë§íˆ¬ ìƒì„±ì— ì í•©í•˜ë„ë¡ system promptë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”.
1. [ìºë¦­í„° ì •ë³´]ì˜ 'ì„¤ëª…'ê³¼ 'ì„±ê²©'ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ, GPTê°€ ìºë¦­í„°ì˜ ë§íˆ¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ë” ëª…í™•í•˜ê³  ìƒìƒí•˜ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”. ë‹¨, ìƒˆë¡œìš´ ì„¤ì •ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì˜ë¯¸ë¥¼ ë°”ê¾¸ë©´ ì•ˆ ë¼ìš”.
2. ì´ì–´ì„œ í•´ë‹¹ ìºë¦­í„° íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•œ [ë§íˆ¬ ì§€ì‹œì‚¬í•­]ê³¼ [ì£¼ì˜ì‚¬í•­]ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. í‘œí˜„ ë°©ì‹, ë§íˆ¬, ê°ì • ì „ë‹¬ ë°©ì‹ ë“± ë§íˆ¬ì— í•„ìš”í•œ êµ¬ì²´ì ì¸ íŠ¹ì§•ì´ ë“œëŸ¬ë‚˜ì•¼ í•´ìš”.
3. ì „ì²´ ì¶œë ¥ í¬ë§·ì€ ì•„ë˜ì™€ ê°™ì•„ì•¼ í•´ìš”:

ë‹¹ì‹ ì€ ì´ì œ ìºë¦­í„°ì²˜ëŸ¼ ëŒ€í™”í•´ì•¼ í•©ë‹ˆë‹¤.

[ìºë¦­í„° ì •ë³´]
{character_info}

[ë§íˆ¬ ì§€ì‹œì‚¬í•­]
{tone_instruction}

[ì£¼ì˜ì‚¬í•­]
{{ìºë¦­í„° íŠ¹ì„±ì— ë”°ë¼ GPTê°€ ì§ì ‘ íŒë‹¨í•œ ì£¼ì˜ì‚¬í•­}}

ëª¨ë“  ë‚´ìš©ì€ ìºë¦­í„° ë§íˆ¬ ìƒì„±ì„ ìœ„í•œ system prompt ìš©ë„ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ, í˜•ì‹ê³¼ ë§íˆ¬ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•´ì£¼ì„¸ìš”.
""".strip()

        messages = [
            {"role": "system", "content": "ì•„ë˜ ìºë¦­í„° ì •ë³´ë¡œ system prompt ì „ì²´ë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”. ë¬¸ì¥ í‘œí˜„ì€ ë§¤ë„ëŸ½ê³  ì •ë¦¬ëœ ìŠ¤íƒ€ì¼ë¡œ í•´ì£¼ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ]
        
        return await self.chat_completion(
            messages=messages,
            model=model,
            temperature=0.7,
            max_tokens=1000
        )


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_global_openai_client = None


def get_openai_client(**kwargs) -> OpenAIClientWrapper:
    """
    ì „ì—­ OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Args:
        **kwargs: OpenAIClientWrapper ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°
        
    Returns:
        OpenAIClientWrapper: í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
    """
    global _global_openai_client
    
    if _global_openai_client is None:
        _global_openai_client = OpenAIClientWrapper(**kwargs)
    
    return _global_openai_client


# ë¹„ë™ê¸° í¸ì˜ í•¨ìˆ˜ë“¤
async def chat_completion(messages: List[Dict[str, str]], **kwargs) -> str:
    """ë¹„ë™ê¸° í¸ì˜ í•¨ìˆ˜: ì±„íŒ… ì™„ì„± API í˜¸ì¶œ"""
    client = get_openai_client()
    return await client.chat_completion(messages, **kwargs)


async def generate_question(character_info: str, **kwargs) -> str:
    """ë¹„ë™ê¸° í¸ì˜ í•¨ìˆ˜: ì§ˆë¬¸ ìƒì„±"""
    client = get_openai_client()
    return await client.generate_question(character_info, **kwargs)


async def summarize_speech_style(system_prompt: str, **kwargs) -> Dict[str, str]:
    """ë¹„ë™ê¸° í¸ì˜ í•¨ìˆ˜: ë§íˆ¬ ìš”ì•½"""
    client = get_openai_client()
    return await client.summarize_speech_style(system_prompt, **kwargs)