import os
os.environ["VLLM_USE_V1"] = "0" # vLLM v1 ì–´í…ì…˜ ë°±ì—”ë“œ ë¹„í™œì„±í™”
import asyncio
import logging
import time
from typing import Optional, Dict, Any, List

import httpx
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from huggingface_hub import login

from app.models import FineTuningStatus, LoRALoadRequest
from pipeline.speech_generator import SpeechGenerator
from app.utils.adapter_utils import get_base_model_from_adapter
from app.utils.finetuning_utils import create_system_message, convert_qa_data_for_finetuning
from pipeline import fine_custom
import dotenv
import re

dotenv.load_dotenv()

logger = logging.getLogger(__name__)

async def get_available_gpu_memory_mb() -> int:
    """nvidia-smië¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬ (MB)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # nvidia-smi ëª…ë ¹ ì‹¤í–‰
        process = await asyncio.create_subprocess_shell(
            "nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            logger.error(f"nvidia-smi ì‹¤í–‰ ì˜¤ë¥˜: {stderr.decode().strip()}")
            return 0

        # ì¶œë ¥ íŒŒì‹±
        output = stdout.decode().strip()
        free_memory_mb = int(output.split('\n')[0])
        return free_memory_mb
    except FileNotFoundError:
        logger.warning("nvidia-smië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ë©”ëª¨ë¦¬ í™•ì¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return -1 # -1ì€ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒì„ ì˜ë¯¸
    except Exception as e:
        logger.error(f"GPU ë©”ëª¨ë¦¬ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0


from fastapi import HTTPException

# ì „ì—­ ë³€ìˆ˜
engine: AsyncLLMEngine = None
tokenizer = None  # í† í¬ë‚˜ì´ì € ì „ì—­ ë³€ìˆ˜ ì¶”ê°€
loaded_adapters: Dict[str, Dict[str, Any]] = {}
finetuning_tasks: Dict[str, Dict[str, Any]] = {}  # íŒŒì¸íŠœë‹ ì‘ì—… ì €ì¥
finetuning_queue: asyncio.Queue = None # íŒŒì¸íŠœë‹ ì‘ì—…ì„ ìœ„í•œ í

# í™˜ê²½ ë³€ìˆ˜
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
FINETUNING_WEBHOOK_URL = os.getenv("FINETUNING_WEBHOOK_URL")

def get_speech_generator() -> SpeechGenerator:
    """
    SpeechGenerator ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•˜ëŠ” ì˜ì¡´ì„± ì£¼ì… í•¨ìˆ˜.
    OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° HTTPExceptionì„ ë°œìƒì‹œí‚µë‹ˆë‹¤.
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("âŒ Speech Generatorë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        raise HTTPException(
            status_code=503,
            detail="Speech Generator is not available due to missing OPENAI_API_KEY."
        )
    logger.info("âœ… SpeechGenerator ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (for OpenAI API)")
    return SpeechGenerator(api_key=api_key)
FINETUNING_WEBHOOK_URL = os.getenv("FINETUNING_WEBHOOK_URL")

async def send_finetuning_webhook(task_id: str, status: str, hf_model_url: Optional[str] = None, error_message: Optional[str] = None):
    """íŒŒì¸íŠœë‹ ì™„ë£Œ/ì‹¤íŒ¨ ì‹œ ë°±ì—”ë“œ ì„œë²„ë¡œ ì›¹í›… ì „ì†¡"""
    if not FINETUNING_WEBHOOK_URL:
        logger.warning("âš ï¸ FINETUNING_WEBHOOK_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹í›…ì„ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    task = finetuning_tasks.get(task_id)
    if not task:
        logger.error(f"ì›¹í›… ì „ì†¡ ì‹¤íŒ¨: ì‘ì—… {task_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    payload = {
        "task_id": task_id,
        "influencer_id": task["influencer_id"],
        "status": status,
        "hf_model_url": hf_model_url,
        "error_message": error_message
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(FINETUNING_WEBHOOK_URL, json=payload, timeout=30.0)
            response.raise_for_status()
            logger.info(f"âœ… íŒŒì¸íŠœë‹ ì›¹í›… ì „ì†¡ ì„±ê³µ: {task_id}, ìƒíƒœ: {status}, ì‘ë‹µ: {response.status_code}")
    except httpx.RequestError as e:
        logger.error(f"âŒ íŒŒì¸íŠœë‹ ì›¹í›… ì „ì†¡ ì‹¤íŒ¨ (RequestError): {task_id}, {e}")
    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ íŒŒì¸íŠœë‹ ì›¹í›… ì „ì†¡ ì‹¤íŒ¨ (HTTPStatusError): {task_id}, ìƒíƒœ ì½”ë“œ: {e.response.status_code}, ì‘ë‹µ: {e.response.text}")
    except Exception as e:
        logger.error(f"âŒ íŒŒì¸íŠœë‹ ì›¹í›… ì „ì†¡ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {task_id}, {e}")

async def run_finetuning_pipeline(qa_data: List[Dict], system_message: str, 
                                hf_token: str, hf_repo_id: str, training_epochs: int) -> Optional[str]:
    """íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    try:
        logger.info(f"ğŸ”„ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: {hf_repo_id}")
        logger.info(f"ğŸ” íŒŒì´í”„ë¼ì¸ QA ë°ì´í„°: ê°œìˆ˜={len(qa_data)}")
        if qa_data:
            logger.info(f"ğŸ” íŒŒì´í”„ë¼ì¸ ì²« ë²ˆì§¸ ë°ì´í„°: {qa_data[0]}")
        
        # fine_custom.pyì˜ main í•¨ìˆ˜ë¥¼ ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        hf_model_url = await asyncio.to_thread(
            fine_custom.main,
            qa_data=qa_data,
            system_message=system_message,
            hf_token=hf_token,
            hf_repo_id=hf_repo_id,
            training_epochs=training_epochs
        )
        
        if hf_model_url:
            logger.info(f"âœ… íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ: {hf_repo_id}")
            return hf_model_url
        else:
            raise Exception("íŒŒì¸íŠœë‹ ì‹¤í–‰ ì‹¤íŒ¨: ëª¨ë¸ URLì„ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"âŒ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise e

async def execute_finetuning(task_id: str):
    """íŒŒì¸íŠœë‹ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    task = finetuning_tasks.get(task_id)
    if not task:
        return
    
    hf_model_url = None
    error_message = None

    try:
        logger.info(f"ğŸ¯ íŒŒì¸íŠœë‹ ì‹¤í–‰ ì‹œì‘: {task_id}")
        
        # 1. ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„
        task["status"] = FineTuningStatus.PREPARING_DATA.value
        task["updated_at"] = time.time()
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±
        system_message = create_system_message(
            task["influencer_name"], 
            task["personality"], 
            task["style_info"]
        )
        
        # QA ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ë³€í™˜
        qa_data = task["qa_data"]
        is_converted = task.get("is_converted", False)
        
        logger.info(f"ğŸ” QA ë°ì´í„° ë””ë²„ê¹…: ê°œìˆ˜={len(qa_data) if qa_data else 0}, is_converted={is_converted}")
        if qa_data:
            logger.info(f"ğŸ” ì²« ë²ˆì§¸ QA ë°ì´í„° ìƒ˜í”Œ: {qa_data[0]}")
        
        # ì´ë¯¸ ë³€í™˜ëœ ë°ì´í„°ì¸ì§€ í™•ì¸
        if is_converted or (qa_data and isinstance(qa_data[0], dict) and "messages" in qa_data[0]):
            logger.info("ì´ë¯¸ ë³€í™˜ëœ íŒŒì¸íŠœë‹ ë°ì´í„° ì‚¬ìš©")
            finetuning_data = qa_data
        else:
            logger.info("QA ë°ì´í„°ë¥¼ íŒŒì¸íŠœë‹ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
            try:
                finetuning_data = convert_qa_data_for_finetuning(
                    qa_data, 
                    task["influencer_name"],
                    task["personality"],
                    task["style_info"]
                )
            except Exception as e:
                logger.error(f"âŒ QA ë°ì´í„° ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.error(f"âŒ QA ë°ì´í„° ìƒ˜í”Œ: {qa_data[:3] if qa_data else 'None'}")
                raise
        
        # 2. íŒŒì¸íŠœë‹ ì‹¤í–‰
        task["status"] = FineTuningStatus.TRAINING.value
        task["updated_at"] = time.time()
        
        hf_model_url = await run_finetuning_pipeline(
            qa_data=finetuning_data,
            system_message=system_message,
            hf_token=task["hf_token"],
            hf_repo_id=task["hf_repo_id"],
            training_epochs=task["training_epochs"]
        )
        
        if hf_model_url:
            # 3. ì™„ë£Œ
            task["status"] = FineTuningStatus.COMPLETED.value
            task["hf_model_url"] = hf_model_url
            task["updated_at"] = time.time()
            
            logger.info(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ: {task_id} â†’ {hf_model_url}")
            
            # ì™„ë£Œëœ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë¡œë“œ
            try:
                load_request = LoRALoadRequest(
                    model_id=task["hf_repo_id"],
                    hf_repo_name=task["hf_repo_id"],
                    hf_token=task["hf_token"]
                )
                await load_lora_adapter(load_request)
                logger.info(f"ğŸ”„ íŒŒì¸íŠœë‹ ì™„ë£Œ í›„ ì–´ëŒ‘í„° ìë™ ë¡œë“œ: {task['hf_repo_id']}")
            except Exception as e:
                logger.warning(f"âš ï¸ íŒŒì¸íŠœë‹ ì™„ë£Œ í›„ ì–´ëŒ‘í„° ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            raise Exception("íŒŒì¸íŠœë‹ ì‹¤í–‰ ì‹¤íŒ¨: ëª¨ë¸ URLì„ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        task["status"] = FineTuningStatus.FAILED.value
        task["error_message"] = str(e)
        task["updated_at"] = time.time()
        logger.error(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {task_id}, {e}")
        error_message = str(e)
    finally:
        # íŒŒì¸íŠœë‹ ì™„ë£Œ/ì‹¤íŒ¨ ì‹œ ì›¹í›… ì „ì†¡
        await send_finetuning_webhook(
            task_id=task_id,
            status=task["status"],
            hf_model_url=hf_model_url,
            error_message=error_message
        )

async def finetuning_worker():
    """íŒŒì¸íŠœë‹ ì‘ì—…ì„ íì—ì„œ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤"""
    MIN_GPU_MEMORY_MB = 1024 * 10 # 10GB (ì˜ˆì‹œ ê°’, ì‹¤ì œ í•„ìš”í•œ ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)

    while True:
        task_id = await finetuning_queue.get()
        logger.info(f"âš™ï¸ íì—ì„œ íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘: {task_id}")
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        available_memory = await get_available_gpu_memory_mb()
        if available_memory != -1 and available_memory < MIN_GPU_MEMORY_MB:
            logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ({available_memory}MB). ìµœì†Œ {MIN_GPU_MEMORY_MB}MB í•„ìš”. ì‘ì—… {task_id}ë¥¼ ë‹¤ì‹œ íì— ë„£ìŠµë‹ˆë‹¤.")
            await finetuning_queue.put(task_id) # ì‘ì—…ì„ ë‹¤ì‹œ íì— ë„£ìŒ
            finetuning_queue.task_done()
            await asyncio.sleep(60) # 1ë¶„ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œë„
            continue

        try:
            await execute_finetuning(task_id)
        except Exception as e:
            logger.error(f"âŒ íŒŒì¸íŠœë‹ ì›Œì»¤ ì˜¤ë¥˜: {task_id}, {e}")
        finally:
            # ì‘ì—… ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                import torch
                import gc
                
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    logger.info(f"â™¾ï¸ íŒŒì¸íŠœë‹ ì‘ì—… {task_id} í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as cleanup_error:
                logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {cleanup_error}")
            
            finetuning_queue.task_done()

async def initialize_vllm_engine():
    global engine, tokenizer
    logger.info("ğŸš€ vLLM LoRA ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
    
    try:
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (chat template ì‚¬ìš©ì„ ìœ„í•´)
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",
                trust_remote_code=True
            )
            logger.info("âœ… í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as tokenizer_error:
            logger.warning(f"âš ï¸ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {tokenizer_error}")
        
        # vLLM ì—”ì§„ ì´ˆê¸°í™” (GPU í•„ìš”í•˜ë¯€ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ)
        try:
            # tensor_parallel_size ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
            tensor_parallel_size = int(os.getenv('VLLM_TENSOR_PARALLEL_SIZE', '1'))
            
            # vLLMì´ ì‚¬ìš©í•  GPU ID ì„¤ì •
            if tensor_parallel_size > 1:
                # multi-GPU ì‚¬ìš© ì‹œ ì²˜ìŒ Nê°œ GPU ì‚¬ìš©
                vllm_gpu_ids = ','.join(str(i) for i in range(tensor_parallel_size))
                os.environ['VLLM_GPU_IDS'] = vllm_gpu_ids
                logger.info(f"vLLM multi-GPU ëª¨ë“œ: GPU {vllm_gpu_ids} ì‚¬ìš©")
            else:
                # ë‹¨ì¼ GPU ì‚¬ìš©
                vllm_gpu_id = os.getenv('VLLM_GPU_ID', '0')
                os.environ['VLLM_GPU_IDS'] = vllm_gpu_id
                logger.info(f"vLLM ë‹¨ì¼ GPU ëª¨ë“œ: GPU {vllm_gpu_id} ì‚¬ìš©")
            
            engine_args = AsyncEngineArgs(
                model="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",
                max_model_len=2048,
                tensor_parallel_size=tensor_parallel_size,
                trust_remote_code=True,
                gpu_memory_utilization=0.5,
                enable_lora=True,
                max_loras=8,
                max_lora_rank=64,
                lora_extra_vocab_size=256,
                max_cpu_loras=16,
                max_num_seqs=256,
                max_num_batched_tokens=8192,
                disable_log_requests=True,
            )
            
            engine = AsyncLLMEngine.from_engine_args(engine_args)
            logger.info("âœ… vLLM LoRA ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (FastAPI ë‚´ë¶€ ì—”ì§„ ëª¨ë“œ)!")
        except Exception as vllm_error:
            logger.warning(f"âš ï¸ vLLM ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨ (GPU ì—†ìŒ?): {vllm_error}")
            logger.info("ğŸ“ vLLM ì—”ì§„ ì—†ì´ íŒŒì¸íŠœë‹ê³¼ Speech Generatorë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤")
        
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise e

async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ë¹„ë™ê¸° ì—”ì§„ ì´ˆê¸°í™” ë° íŒŒì¸íŠœë‹ ì›Œì»¤ ì‹œì‘"""
    global finetuning_queue
    
    try:
        await initialize_vllm_engine()
        logger.info("âœ… vLLM ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ vLLM ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.warning("âš ï¸ vLLM ì—”ì§„ ì—†ì´ íŒŒì¸íŠœë‹ íë§Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤")

    # vLLM ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨ì™€ ê´€ê³„ì—†ì´ íŒŒì¸íŠœë‹ íëŠ” ì´ˆê¸°í™”
    logger.info("ğŸ”„ íŒŒì¸íŠœë‹ í ì´ˆê¸°í™” ì¤‘...")
    finetuning_queue = asyncio.Queue()
    logger.info(f"âœ… íŒŒì¸íŠœë‹ í ì´ˆê¸°í™” ì™„ë£Œ: {finetuning_queue}")
    
    logger.info("ğŸ”„ íŒŒì¸íŠœë‹ ì›Œì»¤ ì‹œì‘ ì¤‘...")
    asyncio.create_task(finetuning_worker())
    logger.info("âœ… íŒŒì¸íŠœë‹ ì›Œì»¤ ì‹œì‘ë¨")
    
    logger.info(f"ğŸ” ìµœì¢… í™•ì¸ - finetuning_queue: {finetuning_queue}")

async def load_lora_adapter(request: LoRALoadRequest):
    """LoRA ì–´ëŒ‘í„° ë¡œë“œ"""
    if engine is None:
        raise Exception("ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        logger.info(f"ğŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë”© ìš”ì²­ ë°›ìŒ: model_id={request.model_id}, repo={request.hf_repo_name}")
        
        if request.model_id in loaded_adapters:
            logger.info(f"â™»ï¸ ì–´ëŒ‘í„° {request.model_id}ëŠ” ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return {
                "message": f"ì–´ëŒ‘í„° {request.model_id}ëŠ” ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                "adapter_info": loaded_adapters[request.model_id]
            }
        
        logger.info(f"ğŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë”© ì‹œì‘: {request.hf_repo_name}")
        
        if request.hf_token:
            try:
                logger.info("ğŸ”‘ í—ˆê¹…í˜ì´ìŠ¤ í† í° ë¡œê·¸ì¸ ì‹œë„ ì¤‘...")
                login(token=request.hf_token)
                logger.info("ğŸ”‘ í—ˆê¹…í˜ì´ìŠ¤ í† í° ë¡œê·¸ì¸ ì„±ê³µ")
            except Exception as e:
                logger.warning(f"âš ï¸ í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
                # í† í° ë¡œê·¸ì¸ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        else:
            logger.info("â„¹ï¸ HuggingFace í† í°ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logger.info("ğŸ“‹ ë² ì´ìŠ¤ ëª¨ë¸ ì •ë³´ í™•ì¸ ì¤‘...")
        try:
            base_model_name = (
                request.base_model_override or 
                get_base_model_from_adapter(request.hf_repo_name, request.hf_token)
            )
            logger.info(f"ğŸ“‹ ë² ì´ìŠ¤ ëª¨ë¸ í™•ì¸ ì™„ë£Œ: {base_model_name}")
        except Exception as e:
            logger.error(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
            raise Exception(f"ë² ì´ìŠ¤ ëª¨ë¸ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        
        logger.info("ğŸ“¦ ì–´ëŒ‘í„° ì •ë³´ ê°ì²´ ìƒì„± ì¤‘...")
        adapter_info = {
            "model_id": request.model_id,
            "hf_repo_name": request.hf_repo_name,
            "base_model_name": base_model_name,
            "status": "loaded",
            "lora_int_id": hash(request.model_id) % 1000000
        }
        logger.info(f"ğŸ“¦ ìƒì„±ëœ ì–´ëŒ‘í„° ì •ë³´: {adapter_info}")
        
        logger.info("ğŸ’¾ ë¡œë“œëœ ì–´ëŒ‘í„° ëª©ë¡ì— ì¶”ê°€ ì¤‘...")
        loaded_adapters[request.model_id] = adapter_info
        logger.info(f"ğŸ’¾ í˜„ì¬ ë¡œë“œëœ ì–´ëŒ‘í„° ëª©ë¡: {list(loaded_adapters.keys())}")
        
        logger.info(f"âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ: {request.model_id}")
        
        result = {
            "message": f"LoRA ì–´ëŒ‘í„° {request.model_id} ë¡œë“œ ì™„ë£Œ",
            "adapter_info": adapter_info
        }
        logger.info(f"ğŸ“¤ ë°˜í™˜í•  ê²°ê³¼: {result}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise Exception(f"ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")