# VLLM μ„λ²„ - AI μΈν”λ£¨μ–Έμ„ GPU μ¶”λ΅  & νμΈνλ‹

μ΄ VLLM μ„λ²„λ” κΈ°μ΅΄ FastAPI λ°±μ—”λ“μ GPU μ¶”λ΅ κ³Ό νμΈνλ‹ λ΅μ§μ„ κ³ μ„±λ¥μΌλ΅ μ²λ¦¬ν•λ” μ „μ© μ„λ²„μ…λ‹λ‹¤.

## π€ μ£Όμ” κΈ°λ¥

### GPU μ¶”λ΅ 
- **λΉ„λ™κΈ° GPU μ¶”λ΅ **: VLLMμ AsyncLLMEngineμ„ μ‚¬μ©ν• κ³ μ„±λ¥ μ¶”λ΅ 
- **LoRA μ–΄λ‘ν„° μ§€μ›**: μµλ€ 8κ°μ LoRA μ–΄λ‘ν„° λ™μ‹ λ΅λ“
- **WebSocket μ±„ν…**: μ‹¤μ‹κ°„ μ±„ν… μΈν„°νμ΄μ¤
- **λ©”λ¨λ¦¬ μµμ ν™”**: 80% GPU λ©”λ¨λ¦¬ ν™μ©λ¥ λ΅ μµμ ν™”

### νμΈνλ‹
- **QLoRA νμΈνλ‹**: 4λΉ„νΈ μ–‘μν™”λ¥Ό ν†µν• λ©”λ¨λ¦¬ ν¨μ¨μ  νμΈνλ‹
- **Hugging Face ν†µν•©**: μλ™ λ¨λΈ μ—…λ΅λ“ λ° λ°°ν¬
- **λ°±κ·ΈλΌμ΄λ“ μ²λ¦¬**: λΉ„λ™κΈ° νμΈνλ‹ μ‘μ—… μ²λ¦¬
- **μƒνƒ λ¨λ‹ν„°λ§**: μ‹¤μ‹κ°„ νμΈνλ‹ μ§„ν–‰ μƒν™© μ¶”μ 

## π“‹ μ”κµ¬μ‚¬ν•­

- **GPU**: CUDA μ§€μ› GPU (8GB VRAM μ΄μƒ κ¶μ¥)
- **Python**: 3.9 μ΄μƒ
- **CUDA**: 11.8 μ΄μƒ
- **λ©”λ¨λ¦¬**: μ‹μ¤ν… RAM 16GB μ΄μƒ κ¶μ¥

## π› οΈ μ„¤μΉ λ° μ‹¤ν–‰

### 1. μμ΅΄μ„± μ„¤μΉ

```bash
cd vllm
pip install -r requirements.txt
```

### 2. ν™κ²½ λ³€μ μ„¤μ •

`.env` νμΌμ„ μƒμ„±ν•κ³  λ‹¤μ λ³€μλ“¤μ„ μ„¤μ •:

```env
# VLLM μ„λ²„ μ„¤μ •
VLLM_HOST=0.0.0.0
VLLM_PORT=8000

# GPU μ„¤μ •
CUDA_VISIBLE_DEVICES=0
VLLM_USE_V1=0

# λ¨λΈ μ„¤μ •
BASE_MODEL=LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct
MAX_MODEL_LEN=2048
GPU_MEMORY_UTILIZATION=0.8

# LoRA μ„¤μ •
ENABLE_LORA=true
MAX_LORAS=8
MAX_LORA_RANK=64
```

### 3. μ„λ²„ μ‹¤ν–‰

**ν†µν•© vLLM μ„λ²„ μ‹¤ν–‰:**
```bash
cd vllm
python main.py
```

λλ” uvicornμΌλ΅ μ§μ ‘ μ‹¤ν–‰:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

**μ£Όμ” κΈ°λ¥:**
- vLLM κΈ°λ° GPU μ¶”λ΅ 
- LoRA μ–΄λ‘ν„° κ΄€λ¦¬
- νμΈνλ‹ νμ΄ν”„λΌμΈ
- WebSocket μ‹¤μ‹κ°„ μ±„ν…
- Speech Generator API (OpenAI κΈ°λ° ν†¤ λ³€ν• μƒμ„±)

## π API μ—”λ“ν¬μΈνΈ

### κΈ°λ³Έ μ •λ³΄
- `GET /` - μ„λ²„ μƒνƒ ν™•μΈ
- `GET /stats` - μ„λ²„ ν†µκ³„ μ •λ³΄

### LoRA μ–΄λ‘ν„° κ΄€λ¦¬
- `POST /load_adapter` - LoRA μ–΄λ‘ν„° λ΅λ“
- `GET /adapters` - λ΅λ“λ μ–΄λ‘ν„° λ©λ΅
- `DELETE /adapter/{model_id}` - μ–΄λ‘ν„° μ–Έλ΅λ“

### GPU μ¶”λ΅ 
- `POST /generate` - ν…μ¤νΈ μƒμ„±
- `WS /ws/chat/{lora_repo}` - WebSocket μ±„ν…

### νμΈνλ‹
- `POST /finetuning/start` - νμΈνλ‹ μ‹μ‘
- `GET /finetuning/status/{task_id}` - νμΈνλ‹ μƒνƒ μ΅°ν
- `GET /finetuning/tasks` - νμΈνλ‹ μ‘μ—… λ©λ΅

### Speech Generator (OpenAI κΈ°λ°)
- `POST /generate_qa` - μΊλ¦­ν„° ν”„λ΅ν•„ κΈ°λ° Q&A λ° ν†¤ λ³€ν• μƒμ„±

## π”§ FastAPI λ°±μ—”λ“ ν†µν•©

### 1. VLLM ν΄λΌμ΄μ–ΈνΈ μ„λΉ„μ¤

λ°±μ—”λ“μ— `app/services/vllm_client.py`κ°€ μ¶”κ°€λμ–΄ VLLM μ„λ²„μ™€ ν†µμ‹ ν•©λ‹λ‹¤:

```python
from app.services.vllm_client import get_vllm_client

# μ–΄λ‘ν„° λ΅λ“
vllm_client = await get_vllm_client()
await vllm_client.load_adapter("model_id", "hf_repo_name", "hf_token")

# μ‘λ‹µ μƒμ„±
result = await vllm_client.generate_response(
    user_message="μ•λ…•ν•μ„Έμ”!",
    influencer_name="AIμΈν”λ£¨μ–Έμ„",
    model_id="model_id"
)
```

### 2. λ°±μ—”λ“ μ„¤μ •

`app/core/config.py`μ— VLLM μ„¤μ • μ¶”κ°€:

```python
# VLLM μ„λ²„ μ„¤μ •
VLLM_HOST: str = "localhost"
VLLM_PORT: int = 8000
VLLM_TIMEOUT: int = 300
VLLM_ENABLED: bool = True
```

### 3. ν΄λ°± λ©”μ»¤λ‹μ¦

VLLM μ„λ²„κ°€ μ‚¬μ© λ¶κ°€λ¥ν• κ²½μ° μλ™μΌλ΅ λ΅μ»¬ λ¨λΈλ΅ ν΄λ°±:

1. **WebSocket**: VLLM μ„λ²„ β†’ λ΅μ»¬ transformers λ¨λΈ
2. **νμΈνλ‹**: VLLM μ„λ²„ β†’ λ΅μ»¬ pipeline/fine_custom.py
3. **μ–΄λ‘ν„° λ΅λ“**: VLLM μ„λ²„ β†’ λ΅μ»¬ PEFT λ¨λΈ μΊμ‹±

## π“ μ„±λ¥ μµμ ν™”

### GPU λ©”λ¨λ¦¬ κ΄€λ¦¬
- **GPU λ©”λ¨λ¦¬ ν™μ©λ¥ **: 80% (μ„¤μ • κ°€λ¥)
- **λ°°μΉ μ²λ¦¬**: μµλ€ 256κ° μ‹ν€€μ¤ λ™μ‹ μ²λ¦¬
- **ν† ν° λ°°μΉ**: μµλ€ 8192 ν† ν°

### LoRA μ–΄λ‘ν„° μµμ ν™”
- **λ™μ‹ λ΅λ“**: μµλ€ 8κ° μ–΄λ‘ν„°
- **λ©”λ¨λ¦¬ κ³µμ **: λ² μ΄μ¤ λ¨λΈ κ³µμ λ΅ λ©”λ¨λ¦¬ μ μ•½
- **μΊμ‹±**: μμ£Ό μ‚¬μ©λλ” μ–΄λ‘ν„° λ©”λ¨λ¦¬ μΊμ‹±

### λΉ„λ™κΈ° μ²λ¦¬
- **AsyncLLMEngine**: λΉ„λ™κΈ° μ¶”λ΅  μ—”μ§„
- **λ°±κ·ΈλΌμ΄λ“ μ‘μ—…**: νμΈνλ‹ λΉ„λ™κΈ° μ²λ¦¬
- **WebSocket**: μ‹¤μ‹κ°„ ν†µμ‹ 

## π” λ¨λ‹ν„°λ§ λ° λ””λ²„κΉ…

### λ΅κ·Έ ν™•μΈ
```bash
# μ‹¤μ‹κ°„ λ΅κ·Έ ν™•μΈ
tail -f vllm_server.log

# μ—λ¬ λ΅κ·Έλ§ ν™•μΈ
grep ERROR vllm_server.log
```

### GPU μ‚¬μ©λ¥  λ¨λ‹ν„°λ§
```bash
# GPU μƒνƒ ν™•μΈ
nvidia-smi

# μ§€μ†μ μΈ λ¨λ‹ν„°λ§
watch -n 1 nvidia-smi
```

### API ν…μ¤νΈ
```bash
# μ„λ²„ μƒνƒ ν™•μΈ
curl http://localhost:8000/

# ν†µκ³„ μ •λ³΄ ν™•μΈ
curl http://localhost:8000/stats

# μ–΄λ‘ν„° λ©λ΅ ν™•μΈ
curl http://localhost:8000/adapters
```

## π¨ λ¬Έμ  ν•΄κ²°

### μΌλ°μ μΈ λ¬Έμ λ“¤

1. **CUDA Out of Memory**
   - `GPU_MEMORY_UTILIZATION` κ°’μ„ λ‚®μ¶¤ (0.7 λλ” 0.6)
   - `MAX_MODEL_LEN` κ°’μ„ μ¤„μ„
   - λ΅λ“λ μ–΄λ‘ν„° μλ¥Ό μ¤„μ„

2. **LoRA μ–΄λ‘ν„° λ΅λ“ μ‹¤ν¨**
   - Hugging Face ν† ν° ν™•μΈ
   - μ–΄λ‘ν„° μ €μ¥μ† μ ‘κ·Ό κ¶ν• ν™•μΈ
   - λ² μ΄μ¤ λ¨λΈ νΈν™μ„± ν™•μΈ

3. **WebSocket μ—°κ²° μ‹¤ν¨**
   - ν¬νΈ μ¶©λ ν™•μΈ (8000λ² ν¬νΈ)
   - λ°©ν™”λ²½ μ„¤μ • ν™•μΈ
   - CORS μ„¤μ • ν™•μΈ

### λ΅κ·Έ λ λ²¨ μ΅°μ •
```python
# main.pyμ—μ„ λ΅κ·Έ λ λ²¨ λ³€κ²½
logging.basicConfig(level=logging.DEBUG)  # λ” μμ„Έν• λ΅κ·Έ
logging.basicConfig(level=logging.WARNING)  # κ²½κ³ λ§ ν‘μ‹
```

## π”„ μ—…κ·Έλ μ΄λ“ λ° μ μ§€λ³΄μ

### μ •κΈ° μ—…λ°μ΄νΈ
```bash
# VLLM μ—…λ°μ΄νΈ
pip install --upgrade vllm

# μ „μ²΄ μμ΅΄μ„± μ—…λ°μ΄νΈ
pip install --upgrade -r requirements.txt
```

### λ¨λΈ μΊμ‹ μ •λ¦¬
```bash
# Hugging Face μΊμ‹ μ •λ¦¬
rm -rf ~/.cache/huggingface/

# μ„μ‹ νμΌ μ •λ¦¬
rm -rf /tmp/vllm_*
```

## π“ μ§€μ›

- **GitHub Issues**: λ²„κ·Έ λ¦¬ν¬νΈ λ° κΈ°λ¥ μ”μ²­
- **Documentation**: API λ¬Έμ„λ” `/docs` μ—”λ“ν¬μΈνΈμ—μ„ ν™•μΈ
- **Logs**: μƒμ„Έν• μ—λ¬ λ΅κ·Έλ” μ„λ²„ λ΅κ·Έ νμΌ ν™•μΈ