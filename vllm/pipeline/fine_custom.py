# EXAONE 3.5 2.4B LoRA íŒŒì¸íŠœë‹ ì˜ˆì‹œ ì½”ë“œ (ìˆ˜ì •ë¨)

import torch
import json
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import Dataset
from huggingface_hub import HfApi
import os
import logging

logger = logging.getLogger(__name__)

# GPU ì„¤ì • í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Hugging Face í† í° ë° repo_id ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)


class ExaoneDataPreprocessor:
    def __init__(self, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def create_chat_format(self, instruction, output, system_msg: str):
        """EXAONE ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜"""
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": output}
        ]
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        formatted_text = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=False
        )
        
        return formatted_text
    
    def tokenize_function(self, examples):
        """í† í°í™” í•¨ìˆ˜"""
        # ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=self.max_length,
            return_tensors=None
        )
        
        # labelsì„ input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (causal LM)
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized

def find_all_linear_names(model):
    """ëª¨ë¸ì—ì„œ ëª¨ë“  Linear ë ˆì´ì–´ ì´ë¦„ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    cls = torch.nn.Linear
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[-1])
    
    # íŠ¹ì • ëª¨ë“ˆë“¤ì€ ì œì™¸ (ì¼ë°˜ì ìœ¼ë¡œ LoRAì— í¬í•¨í•˜ì§€ ì•ŠìŒ)
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    if 'embed_tokens' in lora_module_names:
        lora_module_names.remove('embed_tokens')
    
    return list(lora_module_names)

def load_model_and_tokenizer(model_name="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    
    # GPU í• ë‹¹ í™•ì¸
    from pipeline.gpu_utils import find_available_gpu, log_gpu_status
    
    # í˜„ì¬ GPU ìƒíƒœ ë¡œê¹…
    log_gpu_status()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì°¾ê¸°
    available_gpu = find_available_gpu(min_memory_mb=10240)  # 10GB ì´ìƒ ì—¬ìœ  ë©”ëª¨ë¦¬
    
    if available_gpu is not None:
        # íŠ¹ì • GPUë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
        os.environ['CUDA_VISIBLE_DEVICES'] = str(available_gpu)
        print(f"íŒŒì¸íŠœë‹ì— GPU {available_gpu} ì‚¬ìš©")
        device_map = "auto"  # ë‹¨ì¼ GPUì—ì„œ autoëŠ” ì „ì²´ ëª¨ë¸ì„ í•´ë‹¹ GPUì— ë¡œë“œ
    else:
        # ì‚¬ìš© ê°€ëŠ¥í•œ GPUê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë™ì‘
        print("ê²½ê³ : ì—¬ìœ  ìˆëŠ” GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        device_map = "auto"
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # íŒ¨ë”© í† í° ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # ëª¨ë¸ ë¡œë“œ - gradient checkpointing ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ìˆ˜ì •ëœ ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=device_map,
        use_cache=False,  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ê³¼ í˜¸í™˜ì„±ì„ ìœ„í•´
    )
    
    # gradient checkpointingì„ ì—¬ê¸°ì„œ ë¨¼ì € í™œì„±í™”
    model.gradient_checkpointing_enable()
    
    # ëª¨ë¸ì„ LoRA í›ˆë ¨ì— ë§ê²Œ ì¤€ë¹„ (gradient checkpointing í›„ì—)
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    
    return model, tokenizer

def setup_lora_config(model):
    """LoRA ì„¤ì • - ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ìë™ íƒì§€"""
    
    # ëª¨ë¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Linear ëª¨ë“ˆë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ê¸°
    target_modules = find_all_linear_names(model)
    print(f"ë°œê²¬ëœ Linear ëª¨ë“ˆë“¤: {target_modules}")
    
    # EXAONE ëª¨ë¸ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª¨ë“ˆë“¤ë§Œ ì„ íƒ
    attention_modules = [name for name in target_modules if any(proj in name for proj in ['q_proj', 'k_proj', 'v_proj', 'o_proj'])]
    
    if not attention_modules:
        # attention ëª¨ë“ˆì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì´ë¦„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë°˜ì ì¸ ì´ë¦„ë“¤ ì‹œë„
        common_names = ['query', 'key', 'value', 'dense', 'fc1', 'fc2', 'gate_proj', 'up_proj', 'down_proj']
        attention_modules = [name for name in target_modules if any(common in name for common in common_names)]
    
    if not attention_modules:
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²˜ìŒ ëª‡ ê°œë§Œ ì‚¬ìš©
        attention_modules = target_modules[:4] if len(target_modules) >= 4 else target_modules
    
    print(f"LoRAì— ì‚¬ìš©í•  ëª¨ë“ˆë“¤: {attention_modules}")
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,  # rankë¥¼ ì¤„ì—¬ì„œ ì•ˆì •ì„± í™•ë³´
        lora_alpha=16,  # alphaë„ ì¤„ì„
        lora_dropout=0.05,
        target_modules=attention_modules,
        bias="none",
        use_rslora=False,
    )
    return lora_config

def prepare_dataset(tokenizer, qa_data: list[dict], system_message: str, max_length=1024):  # max_length ì¤„ì„
    """ë°ì´í„°ì…‹ ì¤€ë¹„ (ì˜ˆì‹œ ë°ì´í„°)"""
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    preprocessor = ExaoneDataPreprocessor(tokenizer, max_length)
    
    formatted_data = []
    print(f"prepare_dataset: Received {len(qa_data)} items")
    if qa_data:
        print(f"First item sample: {qa_data[0]}")
    
    for i, item in enumerate(qa_data):
        # ì´ë¯¸ ë³€í™˜ëœ ë°ì´í„°ì¸ì§€ í™•ì¸ (messages í‚¤ê°€ ìˆëŠ” ê²½ìš°)
        if "messages" in item:
            # ì´ë¯¸ ë³€í™˜ëœ í˜•ì‹ì—ì„œ question/answer ì¶”ì¶œ
            messages = item["messages"]
            question = ""
            answer = ""
            
            for msg in messages:
                if msg.get("role") == "user":
                    question = msg.get("content", "")
                elif msg.get("role") == "assistant":
                    answer = msg.get("content", "")
            
            if question and answer:
                formatted_text = preprocessor.create_chat_format(
                    question, 
                    answer,
                    system_msg=system_message
                )
                formatted_data.append({"text": formatted_text})
        else:
            # ì›ì‹œ QA í˜•ì‹
            question = item.get("question", "")
            answer = item.get("answer", "")
            
            if not question and not answer:
                print(f"Warning: Item {i} has no question or answer: {item}")
                continue
            
            if question and answer:
                formatted_text = preprocessor.create_chat_format(
                    question, 
                    answer,
                    system_msg=system_message
                )
                formatted_data.append({"text": formatted_text})
            else:
                print(f"Warning: Item {i} incomplete - question: '{question}', answer: '{answer}'")
    
    # Dataset ê°ì²´ ìƒì„±
    print(f"prepare_dataset: Successfully formatted {len(formatted_data)} items out of {len(qa_data)}")
    
    if not formatted_data:
        raise ValueError("No valid QA data found after formatting. Check data structure.")
    
    dataset = Dataset.from_list(formatted_data)
    
    # í† í°í™”
    tokenized_dataset = dataset.map(
        preprocessor.tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    # ë°ì´í„° ê²€ì¦ ë° ìˆ˜ì •
    def validate_and_fix_data(example):
        """ë°ì´í„° í˜•ì‹ ê²€ì¦ ë° ìˆ˜ì •"""
        # input_idsì™€ labelsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if isinstance(example['input_ids'], list) and isinstance(example['labels'], list):
            # ì •ìƒì ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            return example
        else:
            # ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ìˆ˜ì •
            if not isinstance(example['input_ids'], list):
                example['input_ids'] = example['input_ids'].tolist() if hasattr(example['input_ids'], 'tolist') else [example['input_ids']]
            if not isinstance(example['labels'], list):
                example['labels'] = example['labels'].tolist() if hasattr(example['labels'], 'tolist') else [example['labels']]
            return example
    
    tokenized_dataset = tokenized_dataset.map(validate_and_fix_data)
    
    return tokenized_dataset

def setup_training_arguments(training_epochs: int, output_dir="./exaone-lora-results-system-custom"):
    """í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •"""
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,  # ì¤„ì„
        num_train_epochs=training_epochs,  
        learning_rate=2e-4,  
        lr_scheduler_type="linear",
        warmup_steps=10,  
        logging_steps=5,
        save_strategy="epoch", 
        eval_strategy="epoch", 
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False,  
        bf16=True,
        gradient_checkpointing=False, 
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to="none",
        seed=42,
        optim="adamw_torch",
        max_grad_norm=1.0,
        dataloader_num_workers=0, 
        save_total_limit=1,
    )
    
    return training_args

def upload_to_huggingface(output_dir, hf_token, hf_repo_id):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œ"""
    if not hf_token:
        print("HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return f"https://huggingface.co/{hf_repo_id}"  # í† í°ì´ ì—†ì–´ë„ URLì€ ë°˜í™˜
    
    try:
        print(f"\n=== Hugging Face Hub ì—…ë¡œë“œ ì‹œì‘ ===")
        api = HfApi()
        
        # 1. ì €ì¥ì†Œ ìƒì„±
        print(f"ì €ì¥ì†Œ ìƒì„± ì¤‘: {hf_repo_id}")
        api.create_repo(
            repo_id=hf_repo_id,
            repo_type="model",
            private=False,
            token=hf_token,
            exist_ok=True,
        )
        
        # 2. ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ
        print(f"ëª¨ë¸ ì—…ë¡œë“œ ì¤‘: {output_dir} -> {hf_repo_id}")
        api.upload_folder(
            repo_id=hf_repo_id,
            folder_path=output_dir,
            repo_type="model",
            token=hf_token,
        )
        
        print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! ëª¨ë¸ URL: https://huggingface.co/{hf_repo_id}")
        
        # 3. ë¡œì»¬ í´ë” ì‚­ì œ
        import shutil
        try:
            print(f"ğŸ—‘ï¸ ë¡œì»¬ í´ë” ì‚­ì œ ì¤‘: {output_dir}")
            shutil.rmtree(output_dir)
            print(f"âœ… ë¡œì»¬ í´ë” ì‚­ì œ ì™„ë£Œ: {output_dir}")
        except Exception as cleanup_error:
            print(f"âš ï¸ ë¡œì»¬ í´ë” ì‚­ì œ ì‹¤íŒ¨: {cleanup_error}")
            # ì‚­ì œ ì‹¤íŒ¨í•´ë„ ì—…ë¡œë“œëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
        
        return f"https://huggingface.co/{hf_repo_id}"
        
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return f"https://huggingface.co/{hf_repo_id}"  # ì‹¤íŒ¨í•´ë„ URLì€ ë°˜í™˜

def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    import gc
    
    # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    gc.collect()
    
    # PyTorch GPU ìºì‹œ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print("âœ… GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

def main(qa_data: list[dict], system_message: str, hf_token: str, hf_repo_id: str, training_epochs: int) -> str:
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_gpu_memory()
    
    # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer()
    
    # 2. ëª¨ë¸ êµ¬ì¡° í™•ì¸
    print("ëª¨ë¸ êµ¬ì¡° í™•ì¸ ì¤‘...")
    print(f"ëª¨ë¸ íƒ€ì…: {type(model)}")
    
    # 3. LoRA ì„¤ì • ë° ì ìš©
    lora_config = setup_lora_config(model)
    model = get_peft_model(model, lora_config)
    
    # 4. í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
    model.print_trainable_parameters()
    
    # 7. ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset = prepare_dataset(tokenizer, qa_data, system_message)
    print(f"í›ˆë ¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")
    
    # ë°ì´í„°ì…‹ì„ train/evalë¡œ ë¶„í•  (ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ validation ë°ì´í„° í•„ìš”)
    train_size = int(0.8 * len(train_dataset))
    eval_size = len(train_dataset) - train_size
    
    train_dataset_split = train_dataset.select(range(train_size))
    eval_dataset = train_dataset.select(range(train_size, train_size + eval_size))
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset_split)}, ê²€ì¦ ë°ì´í„°: {len(eval_dataset)}")
    
    # 8. ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • - ë” ì•ˆì „í•œ ë°©ì‹
    def data_collator(features):
        """ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„°"""
        # ì…ë ¥ ê¸¸ì´ í™•ì¸
        max_length = max(len(f["input_ids"]) for f in features)
        
        batch = {
            "input_ids": [],
            "attention_mask": [],
            "labels": []
        }
        
        for feature in features:
            input_ids = feature["input_ids"]
            labels = feature["labels"]
            
            # íŒ¨ë”© ì¶”ê°€
            padding_length = max_length - len(input_ids)
            
            # input_ids íŒ¨ë”©
            padded_input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
            
            # attention_mask ìƒì„±
            attention_mask = [1] * len(input_ids) + [0] * padding_length
            
            # labels íŒ¨ë”© (-100ì€ loss ê³„ì‚°ì—ì„œ ë¬´ì‹œë¨)
            padded_labels = labels + [-100] * padding_length
            
            batch["input_ids"].append(padded_input_ids)
            batch["attention_mask"].append(attention_mask)
            batch["labels"].append(padded_labels)
        
        # í…ì„œë¡œ ë³€í™˜
        return {
            "input_ids": torch.tensor(batch["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(batch["attention_mask"], dtype=torch.long),
            "labels": torch.tensor(batch["labels"], dtype=torch.long)
        }
    
    # 9. í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •
    training_args = setup_training_arguments(training_epochs)
    
    # 10. ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •
    early_stopping_callback = EarlyStoppingCallback(
        early_stopping_patience=2,  # 2 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
        early_stopping_threshold=0.01  # ìµœì†Œ ê°œì„  ì„ê³„ê°’
    )
    
    # 11. Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset_split,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        callbacks=[early_stopping_callback],
    )
    
    # 12. í›ˆë ¨ ì‹œì‘
    print("í›ˆë ¨ ì‹œì‘...")
    try:
        trainer.train()
        print("í›ˆë ¨ ì™„ë£Œ!")
    except Exception as e:
        print(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´
        print("\n=== ì¶”ê°€ ë””ë²„ê¹… ì •ë³´ ===")
        print(f"ëª¨ë¸ íƒ€ì…: {type(model)}")
        print(f"Base model íƒ€ì…: {type(model.base_model) if hasattr(model, 'base_model') else 'N/A'}")
        
        # PEFT ì„¤ì • í™•ì¸
        if hasattr(model, 'peft_config'):
            print(f"PEFT config: {model.peft_config}")
        
        raise
    
    # 14. ëª¨ë¸ ì €ì¥
    trainer.save_model()
    print(f"ëª¨ë¸ì´ {training_args.output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # 15. Hugging Face Hubì— ì—…ë¡œë“œ
    hf_model_url = upload_to_huggingface(training_args.output_dir, hf_token, hf_repo_id)
    
    # 16. ëª¨ë¸ê³¼ íŠ¸ë ˆì´ë„ˆ ë©”ëª¨ë¦¬ í•´ì œ
    print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
    try:
        # ëª¨ë¸ì„ CPUë¡œ ì´ë™ í›„ ì‚­ì œ
        if hasattr(model, 'cpu'):
            model.cpu()
        del model
        del trainer
        del tokenizer
        if 'train_dataset' in locals():
            del train_dataset
        if 'train_dataset_split' in locals():
            del train_dataset_split
        if 'eval_dataset' in locals():
            del eval_dataset
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_gpu_memory()
        
        print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    
    # 17. HuggingFace ëª¨ë¸ URL ë°˜í™˜
    print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ URL: {hf_model_url}")
    return hf_model_url

