from fastapi import APIRouter, Depends, Query, BackgroundTasks, HTTPException
from sqlalchemy.orm import Session
from typing import List, Optional
import os
import logging
import json
from app.database import get_db
from app.schemas.influencer import (
    AIInfluencer as AIInfluencerSchema,
    AIInfluencerWithDetails,
    AIInfluencerCreate,
    AIInfluencerUpdate,
    StylePreset as StylePresetSchema,
    StylePresetCreate,
    ModelMBTI as ModelMBTISchema,
    FinetuningWebhookRequest,
    ToneGenerationRequest,
    SystemPromptSaveRequest,
)
from app.core.security import get_current_user
from app.services.influencers.crud import (
    get_influencers_list,
    get_influencer_by_id,
    create_influencer,
    update_influencer,
    delete_influencer,
)
from app.services.influencers.style_presets import (
    get_style_presets,
    create_style_preset,
)
from app.services.influencers.mbti import get_mbti_list
from app.services.influencers.instagram import (
    InstagramConnectRequest,
    connect_instagram_account,
    disconnect_instagram_account,
    get_instagram_status,
)
from app.services.background_tasks import (
    generate_influencer_qa_background,
    get_background_task_manager,
    BackgroundTaskManager,
)
from fastapi import Request, status
from app.services.influencers.qa_generator import QAGenerationStatus
from app.services.finetuning_service import (
    get_finetuning_service,
    InfluencerFineTuningService,
)
from datetime import datetime
from app.models.influencer import StylePreset, BatchKey, AIInfluencer
from fastapi import HTTPException
from typing import Dict, Any
from openai import OpenAI
import os
import json
from pydantic import BaseModel

router = APIRouter()
logger = logging.getLogger(__name__)


# ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ê´€ë ¨ API (êµ¬ì²´ì ì¸ ê²½ë¡œë¥¼ ë¨¼ì € ì •ì˜)
@router.get("/style-presets", response_model=List[StylePresetSchema])
async def get_style_presets_list(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ëª©ë¡ ì¡°íšŒ"""
    logger.info(f"ğŸ¯ ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ëª©ë¡ ì¡°íšŒ API í˜¸ì¶œë¨ - skip: {skip}, limit: {limit}")
    try:
        # StylePreset ëª¨ë¸ë§Œ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë¬¸ì œ íšŒí”¼
        from app.models.influencer import StylePreset
        presets = db.query(StylePreset).offset(skip).limit(limit).all()
        logger.info(f"âœ… í”„ë¦¬ì…‹ ì¡°íšŒ ì„±ê³µ - ê°œìˆ˜: {len(presets)}")
        return presets
    except Exception as e:
        logger.error(f"âŒ í”„ë¦¬ì…‹ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í”„ë¦¬ì…‹ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.post("/style-presets", response_model=StylePresetSchema)
async def create_new_style_preset(
    preset_data: StylePresetCreate,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìƒˆ ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ìƒì„±"""
    return create_style_preset(db, preset_data)


@router.get("/style-presets/{style_preset_id}", response_model=StylePresetSchema)
async def get_style_preset_by_id(
    style_preset_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """íŠ¹ì • ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ ë‹¨ì¼ ì¡°íšŒ"""
    preset = (
        db.query(StylePreset)
        .filter(StylePreset.style_preset_id == style_preset_id)
        .first()
    )
    if not preset:
        raise HTTPException(status_code=404, detail="StylePreset not found")
    return preset


# MBTI ê´€ë ¨ API
@router.get("/mbti", response_model=List[ModelMBTISchema])
async def get_mbti_options(
    db: Session = Depends(get_db), current_user: dict = Depends(get_current_user)
):
    """MBTI ëª©ë¡ ì¡°íšŒ"""
    return get_mbti_list(db)


# ì¸í”Œë£¨ì–¸ì„œ ê´€ë ¨ API
@router.get("", response_model=List[AIInfluencerSchema])
async def get_influencers(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ì‚¬ìš©ìë³„ AI ì¸í”Œë£¨ì–¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")

    return get_influencers_list(db, user_id, skip, limit)


@router.get("/{influencer_id}", response_model=AIInfluencerWithDetails)
async def get_influencer(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """íŠ¹ì • AI ì¸í”Œë£¨ì–¸ì„œ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    return get_influencer_by_id(db, user_id, influencer_id)


@router.post("", response_model=AIInfluencerSchema)
async def createnew_influencer(
    influencer_data: AIInfluencerCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ìƒˆ AI ì¸í”Œë£¨ì–¸ì„œ ìƒì„±"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    
    logger.info(f"ğŸš€ API: ì¸í”Œë£¨ì–¸ì„œ ìƒì„± ìš”ì²­ - user_id: {user_id}, name: {influencer_data.influencer_name}")
    
    # ì¸í”Œë£¨ì–¸ì„œ ìƒì„±
    influencer = create_influencer(db, user_id, influencer_data)

    # í™˜ê²½ë³€ìˆ˜ë¡œ ìë™ QA ìƒì„± ì œì–´
    auto_qa_enabled = os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"
    logger.info(f"ğŸ”§ ìë™ QA ìƒì„± ì„¤ì •: {auto_qa_enabled}")

    if auto_qa_enabled:
        logger.info(
            f"âš¡ ë°±ê·¸ë¼ìš´ë“œ QA ìƒì„± ì‘ì—… ì‹œì‘ - influencer_id: {influencer.influencer_id}"
        )
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ QA ìƒì„± ì‘ì—… ì‹œì‘
        background_tasks.add_task(
            generate_influencer_qa_background, influencer.influencer_id, user_id
        )
    else:
        logger.info("â¸ï¸ ìë™ QA ìƒì„±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")

    logger.info(f"âœ… API: ì¸í”Œë£¨ì–¸ì„œ ìƒì„± ì™„ë£Œ - ID: {influencer.influencer_id}")
    return influencer


@router.put("/{influencer_id}", response_model=AIInfluencerSchema)
async def update_existing_influencer(
    influencer_id: str,
    influencer_update: AIInfluencerUpdate,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ìˆ˜ì •"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    return update_influencer(db, user_id, influencer_id, influencer_update)

@router.delete("/{influencer_id}")
async def delete_existing_influencer(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œ ì‚­ì œ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    return delete_influencer(db, user_id, influencer_id)

# Instagram ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì • ì—°ë™ ê´€ë ¨ API
@router.post("/{influencer_id}/instagram/connect")
async def connect_instagram_business(
    influencer_id: str,
    request: InstagramConnectRequest,
    req: Request,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì— Instagram ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì • ì—°ë™"""
    # ì›ì‹œ ìš”ì²­ ë°ì´í„° í™•ì¸
    try:
        body = await req.json()
        print(f"ğŸ” DEBUG Raw request body: {body}")
    except:
        print("ğŸ” DEBUG Failed to parse request body")
    
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    print(f"ğŸ” DEBUG influencer_id: {influencer_id}")
    print(f"ğŸ” DEBUG request: {request}")
    print(f"ğŸ” DEBUG request.code: {request.code}")
    print(f"ğŸ” DEBUG request.redirect_uri: {request.redirect_uri}")
    return await connect_instagram_account(db, user_id, influencer_id, request)


@router.delete("/{influencer_id}/instagram/disconnect")
async def disconnect_instagram_business(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì—ì„œ Instagram ë¹„ì¦ˆë‹ˆìŠ¤ ê³„ì • ì—°ë™ í•´ì œ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    return disconnect_instagram_account(db, user_id, influencer_id)


@router.get("/{influencer_id}/instagram/status")
async def get_instagram_connection_status(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ Instagram ì—°ë™ ìƒíƒœ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    return await get_instagram_status(db, user_id, influencer_id)


# QA ìƒì„± ê´€ë ¨ API
@router.post("/{influencer_id}/qa/generate")
async def trigger_qa_generation(
    influencer_id: str,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ QA ìƒì„± ìˆ˜ë™ íŠ¸ë¦¬ê±°"""
    user_id = current_user.get("sub")

    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    
    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:

        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í™˜ê²½ë³€ìˆ˜ë¡œ ìë™ QA ìƒì„± ì œì–´
    auto_qa_enabled = os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"

    if not auto_qa_enabled:
        raise HTTPException(status_code=403, detail="ìë™ QA ìƒì„±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ QA ìƒì„± ì‘ì—… ì‹œì‘
    background_tasks.add_task(generate_influencer_qa_background, influencer_id)

    return {"message": "QA ìƒì„± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤", "influencer_id": influencer_id}


@router.get("/{influencer_id}/qa/status")
async def get_qa_generation_status(
    influencer_id: str,
    task_id: Optional[str] = Query(None, description="íŠ¹ì • ì‘ì—… IDë¡œ ì¡°íšŒ"),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    task_manager: BackgroundTaskManager = Depends(get_background_task_manager),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ QA ìƒì„± ìƒíƒœ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    
    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:
        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    if task_id:
        # íŠ¹ì • ì‘ì—… ìƒíƒœ ì¡°íšŒ (DBì—ì„œ)
        batch_key_entry = db.query(BatchKey).filter(BatchKey.task_id == task_id).first()
        
        if not batch_key_entry or str(batch_key_entry.influencer_id) != influencer_id:
            raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ì‹¤ì‹œê°„ OpenAI ë°°ì¹˜ ìƒíƒœ í™•ì¸
        openai_batch_status = None
        if batch_key_entry.openai_batch_id:
            try:
                openai_batch_status = task_manager.qa_generator.check_batch_status(str(batch_key_entry.openai_batch_id))

            except Exception as e:
                openai_batch_status = {"error": f"OpenAI ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}

        s3_urls = {}
        if batch_key_entry.s3_qa_file_url:
            s3_urls["processed_qa_url"] = str(batch_key_entry.s3_qa_file_url)
        if batch_key_entry.s3_processed_file_url:
            s3_urls["raw_results_url"] = str(batch_key_entry.s3_processed_file_url)

        return {
            "task_id": batch_key_entry.task_id,
            "influencer_id": str(batch_key_entry.influencer_id),
            "status": batch_key_entry.status, # DBì—ì„œ ì§ì ‘ ìƒíƒœ ê°€ì ¸ì˜´
            "batch_id": batch_key_entry.openai_batch_id,
            "total_qa_pairs": batch_key_entry.total_qa_pairs,
            "generated_qa_pairs": batch_key_entry.generated_qa_pairs,
            "error_message": batch_key_entry.error_message,
            "s3_urls": s3_urls,
            "created_at": batch_key_entry.created_at,
            "updated_at": batch_key_entry.updated_at,
            "is_running": batch_key_entry.status in [
                QAGenerationStatus.PENDING.value, 
                QAGenerationStatus.TONE_GENERATION.value,
                QAGenerationStatus.DOMAIN_PREPARATION.value,
                QAGenerationStatus.PROCESSING.value, 
                QAGenerationStatus.BATCH_SUBMITTED.value, 
                QAGenerationStatus.BATCH_PROCESSING.value,
                QAGenerationStatus.BATCH_UPLOAD.value,
                QAGenerationStatus.PROCESSING_RESULTS.value
            ], # DB ìƒíƒœ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ì—¬ë¶€ íŒë‹¨
            "openai_batch_status": openai_batch_status,  # ì‹¤ì œ OpenAI ìƒíƒœ ì¶”ê°€
        }
    else:
        # í•´ë‹¹ ì¸í”Œë£¨ì–¸ì„œì˜ ëª¨ë“  ì‘ì—… ì¡°íšŒ (DBì—ì„œ)
        all_tasks_from_db = db.query(BatchKey).filter(BatchKey.influencer_id == influencer_id).order_by(BatchKey.created_at.desc()).all()
        
        influencer_tasks = [
            {
                "task_id": task.task_id,
                "status": task.status,
                "batch_id": task.openai_batch_id,
                "total_qa_pairs": task.total_qa_pairs,
                "generated_qa_pairs": task.generated_qa_pairs,
                "error_message": task.error_message,
                "s3_urls": {
                    "processed_qa_url": task.s3_qa_file_url,
                    "raw_results_url": task.s3_processed_file_url
                } if task.s3_qa_file_url or task.s3_processed_file_url else None,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
                "is_running": task.status in [
                    QAGenerationStatus.PENDING.value, 
                    QAGenerationStatus.TONE_GENERATION.value,
                    QAGenerationStatus.DOMAIN_PREPARATION.value,
                    QAGenerationStatus.PROCESSING.value, 
                    QAGenerationStatus.BATCH_SUBMITTED.value, 
                    QAGenerationStatus.BATCH_PROCESSING.value,
                    QAGenerationStatus.BATCH_UPLOAD.value,
                    QAGenerationStatus.PROCESSING_RESULTS.value
                ],
            }
            for task in all_tasks_from_db
        ]

        return {
            "influencer_id": influencer_id,
            "tasks": influencer_tasks,
            "total_tasks": len(influencer_tasks),
            "running_tasks": len([t for t in influencer_tasks if t["is_running"]]),
        }


@router.delete("/{influencer_id}/qa/tasks/{task_id}")
async def cancel_qa_generation(
    influencer_id: str,
    task_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    task_manager: BackgroundTaskManager = Depends(get_background_task_manager),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ QA ìƒì„± ì‘ì—… ì·¨ì†Œ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    
    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:
        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ì‘ì—… ì¡´ì¬ í™•ì¸ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
    batch_key_entry = db.query(BatchKey).filter(BatchKey.task_id == task_id).first()
    if not batch_key_entry or batch_key_entry.influencer_id != influencer_id:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ì´ë¯¸ ì™„ë£Œë˜ê±°ë‚˜ ì‹¤íŒ¨í•œ ì‘ì—…ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŒ
    if batch_key_entry.status in [QAGenerationStatus.COMPLETED.value, QAGenerationStatus.FAILED.value, QAGenerationStatus.BATCH_COMPLETED.value]:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ì™„ë£Œë˜ì—ˆê±°ë‚˜ ì‹¤íŒ¨í•œ ì‘ì—…ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ìƒíƒœë¥¼ ì·¨ì†Œë¡œ ë³€ê²½
    batch_key_entry.status = QAGenerationStatus.FAILED.value # ì·¨ì†Œë„ ì‹¤íŒ¨ë¡œ ê°„ì£¼
    batch_key_entry.error_message = "ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨"
    db.commit()

    # TODO: OpenAI ë°°ì¹˜ ì‘ì—… ìì²´ë¥¼ ì·¨ì†Œí•˜ëŠ” ë¡œì§ ì¶”ê°€ í•„ìš” (API ì§€ì› ì‹œ)
    # í˜„ì¬ëŠ” DB ìƒíƒœë§Œ ì—…ë°ì´íŠ¸

    return {
        "message": "ì‘ì—… ì·¨ì†Œ ìš”ì²­ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
        "task_id": task_id,
        "cancelled": True,
    }


@router.get("/qa/tasks/status")
async def get_all_qa_tasks_status(
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ëª¨ë“  QA ìƒì„± ì‘ì—… ìƒíƒœ ì¡°íšŒ (ê´€ë¦¬ììš©)"""
    # ëª¨ë“  BatchKey ì‘ì—… ì¡°íšŒ (DBì—ì„œ)
    all_tasks_from_db = db.query(BatchKey).order_by(BatchKey.created_at.desc()).all()

    tasks_data = [
        {
            "task_id": task.task_id,
            "influencer_id": task.influencer_id,
            "status": task.status,
            "batch_id": task.openai_batch_id,
            "total_qa_pairs": task.total_qa_pairs,
            "generated_qa_pairs": task.generated_qa_pairs,
            "error_message": task.error_message,
            "s3_urls": {
                "processed_qa_url": task.s3_qa_file_url,
                "raw_results_url": task.s3_processed_file_url
            } if task.s3_qa_file_url or task.s3_processed_file_url else None,
            "created_at": task.created_at,
            "updated_at": task.updated_at,
            "is_running": task.status in [
            QAGenerationStatus.PENDING.value, 
            QAGenerationStatus.TONE_GENERATION.value,
            QAGenerationStatus.DOMAIN_PREPARATION.value,
            QAGenerationStatus.PROCESSING.value, 
            QAGenerationStatus.BATCH_SUBMITTED.value, 
            QAGenerationStatus.BATCH_PROCESSING.value,
            QAGenerationStatus.BATCH_UPLOAD.value,
            QAGenerationStatus.PROCESSING_RESULTS.value
        ],
        }
        for task in all_tasks_from_db
    ]

    return {
        "total_tasks": len(tasks_data),
        "running_tasks": len([t for t in tasks_data if t["is_running"]]),
        "tasks": tasks_data,
    }


# íŒŒì¸íŠœë‹ ê´€ë ¨ API
@router.get("/{influencer_id}/finetuning/status")
async def get_finetuning_status(
    influencer_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    finetuning_service: InfluencerFineTuningService = Depends(get_finetuning_service),
):
    """AI ì¸í”Œë£¨ì–¸ì„œì˜ íŒŒì¸íŠœë‹ ìƒíƒœ ì¡°íšŒ"""
    user_id = current_user.get("sub")
    if not user_id:
        raise HTTPException(status_code=401, detail="User ID not found")
    
    # ì¸í”Œë£¨ì–¸ì„œ ì¡´ì¬ í™•ì¸
    influencer = get_influencer_by_id(db, user_id, influencer_id)
    if not influencer:
        raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í•´ë‹¹ ì¸í”Œë£¨ì–¸ì„œì˜ íŒŒì¸íŠœë‹ ì‘ì—… ì¡°íšŒ
    tasks = finetuning_service.get_tasks_by_influencer(influencer_id)

    return {
        "influencer_id": influencer_id,
        "finetuning_tasks": [
            {
                "task_id": task.task_id,
                "qa_task_id": task.qa_task_id,
                "status": task.status.value,
                "model_name": task.model_name,
                "hf_repo_id": task.hf_repo_id,
                "hf_model_url": task.hf_model_url,
                "error_message": task.error_message,
                "training_epochs": task.training_epochs,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
            }
            for task in tasks
        ],
        "total_tasks": len(tasks),
        "latest_task": tasks[-1].__dict__ if tasks else None,
    }


@router.get("/finetuning/tasks/status")
async def get_all_finetuning_tasks_status(
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
    finetuning_service: InfluencerFineTuningService = Depends(get_finetuning_service),
):
    """ëª¨ë“  íŒŒì¸íŠœë‹ ì‘ì—… ìƒíƒœ ì¡°íšŒ (ê´€ë¦¬ììš©)"""
    all_tasks = finetuning_service.get_all_tasks()

    return {
        "total_tasks": len(all_tasks),
        "tasks": [
            {
                "task_id": task.task_id,
                "influencer_id": task.influencer_id,
                "qa_task_id": task.qa_task_id,
                "status": task.status.value,
                "model_name": task.model_name,
                "hf_repo_id": task.hf_repo_id,
                "hf_model_url": task.hf_model_url,
                "error_message": task.error_message,
                "training_epochs": task.training_epochs,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
            }
            for task in all_tasks.values()
        ],
    }


@router.post("/webhooks/openai/batch-complete")
async def handle_openai_batch_webhook(
    request: Request,
    db: Session = Depends(get_db),
):
    """OpenAI ë°°ì¹˜ ì‘ì—… ì™„ë£Œ ì›¹í›… ì²˜ë¦¬"""
    try:
        # ì›¹í›… ë°ì´í„° íŒŒì‹±
        webhook_data = await request.json()

        # ë°°ì¹˜ IDì™€ ìƒíƒœ ì¶”ì¶œ
        batch_id = webhook_data.get("data", {}).get("id")
        batch_status = webhook_data.get("data", {}).get("status")

        if not batch_id:
            return {"error": "ë°°ì¹˜ IDê°€ ì—†ìŠµë‹ˆë‹¤"}

        print(f"ğŸ¯ OpenAI ì›¹í›… ìˆ˜ì‹ : batch_id={batch_id}, status={batch_status}")

        # í•´ë‹¹ ë°°ì¹˜ IDë¥¼ ê°€ì§„ ì‘ì—… ì°¾ê¸° (DBì—ì„œ)
        from app.models.influencer import BatchKey
        batch_key_entry = db.query(BatchKey).filter(BatchKey.openai_batch_id == batch_id).first()

        if not batch_key_entry:
            print(f"âš ï¸ í•´ë‹¹ ë°°ì¹˜ IDë¥¼ ê°€ì§„ BatchKeyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: batch_id={batch_id}")
            return {"error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        print(
            f"âœ… BatchKey ë°œê²¬: task_id={batch_key_entry.task_id}, influencer_id={batch_key_entry.influencer_id}"
        )

        # ë°°ì¹˜ ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì²˜ë¦¬
        if batch_status == "completed":
            print(f"ğŸš€ ë°°ì¹˜ ì™„ë£Œ, ì¦‰ì‹œ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘: task_id={batch_key_entry.task_id}")

            # í™˜ê²½ë³€ìˆ˜ë¡œ ìë™ ì²˜ë¦¬ ì œì–´
            auto_qa_enabled = (
                os.getenv("AUTO_FINETUNING_ENABLED", "true").lower() == "true"
            )

            if not auto_qa_enabled:
                print(
                    f"ğŸ”’ ìë™ QA ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (AUTO_FINETUNING_ENABLED=false)"
                )
                # DB ìƒíƒœë§Œ ì—…ë°ì´íŠ¸
                batch_key_entry.status = QAGenerationStatus.BATCH_COMPLETED.value
                db.commit()
                return {
                    "message": "ìë™ QA ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
                    "task_id": batch_key_entry.task_id,
                }

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            batch_key_entry.status = QAGenerationStatus.BATCH_COMPLETED.value
            db.commit()

            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê²°ê³¼ ì²˜ë¦¬ ë° S3 ì—…ë¡œë“œ ì‹¤í–‰
            import asyncio
            from app.database import get_db
            from app.services.influencers.qa_generator import InfluencerQAGenerator

            async def process_webhook_result():
                """ì›¹í›… ê²°ê³¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³„ë„ DB ì„¸ì…˜ ì‚¬ìš©"""
                webhook_db = next(get_db())
                try:
                    qa_generator_instance = InfluencerQAGenerator() # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    await qa_generator_instance.complete_qa_generation(batch_key_entry.task_id, webhook_db)
                finally:
                    webhook_db.close()

            asyncio.create_task(process_webhook_result())

            return {"message": "ë°°ì¹˜ ì™„ë£Œ ì›¹í›… ì²˜ë¦¬ ì‹œì‘", "task_id": batch_key_entry.task_id}

        elif batch_status == "failed":
            print(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: task_id={batch_key_entry.task_id}")
            batch_key_entry.status = QAGenerationStatus.FAILED.value
            batch_key_entry.error_message = "OpenAI ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨"
            db.commit()

            return {"message": "ë°°ì¹˜ ì‹¤íŒ¨ ì²˜ë¦¬ ì™„ë£Œ", "task_id": batch_key_entry.task_id}

        # ê·¸ ì™¸ ìƒíƒœ (ì˜ˆ: validating, in_progress)ëŠ” DBì— ì—…ë°ì´íŠ¸
        batch_key_entry.status = batch_status
        db.commit()
        return {"message": "ì›¹í›… ìˆ˜ì‹ ", "batch_id": batch_id, "status": batch_status}

    except Exception as e:
        print(f"âŒ ì›¹í›… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback

        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {"error": f"ì›¹í›… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"}


@router.post("/webhooks/finetuning-complete")
async def handle_finetuning_webhook(
    webhook_data: FinetuningWebhookRequest,
    db: Session = Depends(get_db),
):
    """íŒŒì¸íŠœë‹ ì™„ë£Œ ì›¹í›… ì²˜ë¦¬"""
    logger.info(f"ğŸ¯ íŒŒì¸íŠœë‹ ì›¹í›… ìˆ˜ì‹ : task_id={webhook_data.task_id}, status={webhook_data.status}")

    try:
        # VLLM task_idë¡œ ë¨¼ì € ì°¾ê³ , ì—†ìœ¼ë©´ ì¼ë°˜ task_idë¡œ ì°¾ê¸°
        batch_key_entry = db.query(BatchKey).filter(BatchKey.vllm_task_id == webhook_data.task_id).first()
        
        if not batch_key_entry:
            # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ task_idë¡œë„ ê²€ìƒ‰
            batch_key_entry = db.query(BatchKey).filter(BatchKey.task_id == webhook_data.task_id).first()

        if not batch_key_entry:
            logger.warning(f"âš ï¸ í•´ë‹¹ task_idë¥¼ ê°€ì§„ BatchKeyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {webhook_data.task_id}")
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        if webhook_data.status == "completed":
            batch_key_entry.status = QAGenerationStatus.FINALIZED.value
            batch_key_entry.hf_model_url = webhook_data.hf_model_url
            batch_key_entry.completed_at = datetime.now()
            logger.info(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ: task_id={webhook_data.task_id}, ëª¨ë¸ URL={webhook_data.hf_model_url}")
            
            # AIInfluencer ëª¨ë¸ ìƒíƒœë¥¼ ì‚¬ìš© ê°€ëŠ¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            influencer = db.query(AIInfluencer).filter(
                AIInfluencer.influencer_id == batch_key_entry.influencer_id
            ).first()
            
            if influencer:
                influencer.learning_status = 1  # 1: ì‚¬ìš©ê°€ëŠ¥
                if webhook_data.hf_model_url:
                    influencer.influencer_model_repo = webhook_data.hf_model_url
                logger.info(f"âœ… ì¸í”Œë£¨ì–¸ì„œ ëª¨ë¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: influencer_id={batch_key_entry.influencer_id}, status=ì‚¬ìš© ê°€ëŠ¥")
        elif webhook_data.status == "failed":
            batch_key_entry.status = QAGenerationStatus.FAILED.value
            batch_key_entry.error_message = webhook_data.error_message
            batch_key_entry.completed_at = datetime.now()
            logger.error(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: task_id={webhook_data.task_id}, ì˜¤ë¥˜={webhook_data.error_message}")
        else:
            # ê¸°íƒ€ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì˜ˆ: processing, validating ë“±)
            batch_key_entry.status = webhook_data.status
            logger.info(f"ğŸ”„ íŒŒì¸íŠœë‹ ìƒíƒœ ì—…ë°ì´íŠ¸: task_id={webhook_data.task_id}, ìƒíƒœ={webhook_data.status}")
        
        db.commit()
        return {"message": "íŒŒì¸íŠœë‹ ì›¹í›… ì²˜ë¦¬ ì™„ë£Œ", "task_id": webhook_data.task_id, "status": webhook_data.status}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ íŒŒì¸íŠœë‹ ì›¹í›… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        db.rollback()
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"íŒŒì¸íŠœë‹ ì›¹í›… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


# ë§íˆ¬ ìƒì„± ê´€ë ¨ API
@router.post("/generate-tones")
async def generate_conversation_tones(
    request: ToneGenerationRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ì„±ê²© ê¸°ë°˜ ë§íˆ¬ ìƒì„± API"""
    from app.services.tone_service import ToneGenerationService
    return await ToneGenerationService.generate_conversation_tones(request, False)


@router.post("/regenerate-tones")
async def regenerate_conversation_tones(
    request: ToneGenerationRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ë§íˆ¬ ì¬ìƒì„± API"""
    from app.services.tone_service import ToneGenerationService
    return await ToneGenerationService.generate_conversation_tones(request, True)


async def _generate_question_for_character(client: OpenAI, character_info: str, temperature: float = 0.6) -> str:
    """ìºë¦­í„° ì •ë³´ì— ì–´ìš¸ë¦¬ëŠ” ì§ˆë¬¸ì„ GPTê°€ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤."""
    prompt = f"""
ë‹¹ì‹ ì€ ì•„ë˜ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ ìºë¦­í„°ê°€ ê°€ì¥ ì˜ ë“œëŸ¬ë‚  ìˆ˜ ìˆëŠ” ìƒí™©ì´ë‚˜ ì¼ìƒì ì¸ ì§ˆë¬¸ í•˜ë‚˜ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ìºë¦­í„° ì •ë³´]
{character_info}

ì¡°ê±´:
- ì§ˆë¬¸ì€ ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ì§ˆë¬¸ì€ ì¼ìƒì ì¸ ëŒ€í™”ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ì§ˆë¬¸ì˜ ë§íˆ¬ë‚˜ ë‹¨ì–´ ì„ íƒë„ ìºë¦­í„°ê°€ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ ìœ ë„í•´ì£¼ì„¸ìš”.
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ìºë¦­í„° ê¸°ë°˜ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=temperature
    )

    return response.choices[0].message.content.strip()


async def _generate_three_tones(client: OpenAI, character_info: str, question: str, temperature: float = 0.9) -> List[Dict[str, str]]:
    """ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 3ê°€ì§€ ë‹¤ë¥¸ ë§íˆ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    conversation_examples = []
    
    for i in range(3):
        # ê° ë§íˆ¬ì— ëŒ€í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = await _generate_system_prompt_for_tone(client, character_info, i+1)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ëŒ€ë‹µ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            max_tokens=500,
            temperature=temperature
        )
        
        generated_text = response.choices[0].message.content.strip()
        
        # ë§íˆ¬ ìš”ì•½ ìƒì„±
        tone_summary = await _summarize_speech_style(client, system_prompt)
        
        conversation_examples.append({
            "title": tone_summary.get("description", f"ë§íˆ¬ {i+1}"),
            "example": generated_text,
            "tone": tone_summary.get("description", f"ë§íˆ¬ {i+1}"),
            "hashtags": tone_summary.get("hashtags", f"#ë§íˆ¬{i+1}"),
            "system_prompt": system_prompt
        })
    
    return conversation_examples


async def _generate_system_prompt_for_tone(client: OpenAI, character_info: str, tone_variation: int) -> str:
    """ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ë§íˆ¬ì— ëŒ€í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    tone_instructions = {
        1: "ì£¼ì–´ì§„ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²« ë²ˆì§¸ ë…íŠ¹í•˜ê³  ì°½ì˜ì ì¸ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”. ìºë¦­í„°ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ë˜ ì˜ˆìƒì¹˜ ëª»í•œ ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.",
        2: "ì£¼ì–´ì§„ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‘ ë²ˆì§¸ ë…íŠ¹í•˜ê³  ì°½ì˜ì ì¸ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì²« ë²ˆì§¸ì™€ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.",
        3: "ì£¼ì–´ì§„ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¸ ë²ˆì§¸ ë…íŠ¹í•˜ê³  ì°½ì˜ì ì¸ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì•ì˜ ë‘ ê°€ì§€ì™€ëŠ” ì „í˜€ ë‹¤ë¥¸ ì°¸ì‹ í•œ ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”."
    }
    
    tone_instruction = tone_instructions.get(tone_variation, "ìºë¦­í„°ì˜ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•œ ì°½ì˜ì  ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    prompt = f"""
    [ìš”ì²­ ì¡°ê±´]
    ë‹¤ìŒ ìºë¦­í„° ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ GPTì˜ ë§íˆ¬ ìƒì„±ì— ì í•©í•˜ë„ë¡ system promptë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”.
    1. [ìºë¦­í„° ì •ë³´]ì˜ 'ì„¤ëª…'ê³¼ 'ì„±ê²©'ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ, GPTê°€ ìºë¦­í„°ì˜ ë§íˆ¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ë” ëª…í™•í•˜ê³  ìƒìƒí•˜ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”. ë‹¨, ìƒˆë¡œìš´ ì„¤ì •ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì˜ë¯¸ë¥¼ ë°”ê¾¸ë©´ ì•ˆ ë¼ìš”.
    2. ì´ì–´ì„œ í•´ë‹¹ ìºë¦­í„° íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•œ [ë§íˆ¬ ì§€ì‹œì‚¬í•­]ê³¼ [ì£¼ì˜ì‚¬í•­]ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. í‘œí˜„ ë°©ì‹, ë§íˆ¬, ê°ì • ì „ë‹¬ ë°©ì‹ ë“± ë§íˆ¬ì— í•„ìš”í•œ êµ¬ì²´ì ì¸ íŠ¹ì§•ì´ ë“œëŸ¬ë‚˜ì•¼ í•´ìš”.
    3. ì „ì²´ ì¶œë ¥ í¬ë§·ì€ ì•„ë˜ì™€ ê°™ì•„ì•¼ í•´ìš”:

    ë‹¹ì‹ ì€ ì´ì œ ìºë¦­í„°ì²˜ëŸ¼ ëŒ€í™”í•´ì•¼ í•©ë‹ˆë‹¤.

    [ìºë¦­í„° ì •ë³´]
    {character_info}

    [ë§íˆ¬ ì§€ì‹œì‚¬í•­]
    {tone_instruction}

    [ì£¼ì˜ì‚¬í•­]
    {{ìºë¦­í„° íŠ¹ì„±ì— ë”°ë¼ GPTê°€ ì§ì ‘ íŒë‹¨í•œ ì£¼ì˜ì‚¬í•­}}

    ëª¨ë“  ë‚´ìš©ì€ ìºë¦­í„° ë§íˆ¬ ìƒì„±ì„ ìœ„í•œ system prompt ìš©ë„ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ, í˜•ì‹ê³¼ ë§íˆ¬ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•´ì£¼ì„¸ìš”.
    """.strip()

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ì•„ë˜ ìºë¦­í„° ì •ë³´ë¡œ system prompt ì „ì²´ë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”. ë¬¸ì¥ í‘œí˜„ì€ ë§¤ë„ëŸ½ê³  ì •ë¦¬ëœ ìŠ¤íƒ€ì¼ë¡œ í•´ì£¼ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=1000
    )

    return response.choices[0].message.content.strip()




async def _summarize_speech_style(client: OpenAI, system_prompt: str) -> Dict[str, str]:
    """ë§íˆ¬ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ ë§íˆ¬ì˜ íŠ¹ì§•ì„ ìš”ì•½í•©ë‹ˆë‹¤."""
    system_instruction = """
    ì£¼ì–´ì§„ ë§íˆ¬ì˜ system promptë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ ë§íˆ¬ì˜ íŠ¹ì§•ì„ ìš”ì•½í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ì§€ì¼œì„œ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

    í˜•ì‹:
    {
        "hashtags": "#í‚¤ì›Œë“œ1 #í‚¤ì›Œë“œ2 #í‚¤ì›Œë“œ3",
        "description": "ë§íˆ¬ ì„¤ëª… (í•œ ë¬¸ì¥, '~ë§íˆ¬'ë¡œ ëë‚˜ì•¼ í•¨)"
    }

    ì¡°ê±´:
    1. ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ MZ ëŠë‚Œë‚˜ê²Œ í‚¤ì›Œë“œ 3ê°œë¥¼ ìƒì„±í•´ í•´ì‹œíƒœê·¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    2. ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ 'ë§íˆ¬'ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤. ì„œìˆ ì–´ ì—†ì´ ëª…ì‚¬í˜•ìœ¼ë¡œ ëë‚©ë‹ˆë‹¤.
    3. ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”. (ì¶”ê°€ ì„¤ëª… ì—†ì´)
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_instruction},
            {"role": "user", "content": f"ë§íˆ¬ ì§€ì‹œì‚¬í•­:\n{system_prompt}"}
        ],
        max_tokens=200,
        temperature=0.7
    )

    try:
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        logger.error(f"ë§íˆ¬ ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {
            "hashtags": "#GPT #ì‘ë‹µíŒŒì‹± #ì‹¤íŒ¨",
            "description": "ë§íˆ¬ ìš”ì•½ ì‹¤íŒ¨í•œ ë§íˆ¬"
        }


@router.post("/{influencer_id}/system-prompt")
async def save_system_prompt(
    influencer_id: str,
    request: SystemPromptSaveRequest,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user),
):
    """ì„ íƒí•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ AI ì¸í”Œë£¨ì–¸ì„œì— ì €ì¥"""
    user_id = current_user.get("sub")
    
    # ìš”ì²­ ë°ì´í„° ê²€ì¦
    if not request.data or not request.data.strip():
        raise HTTPException(status_code=400, detail="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°ì´í„°ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    if request.type not in ["system", "custom"]:
        raise HTTPException(status_code=400, detail="typeì€ 'system' ë˜ëŠ” 'custom'ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    
    try:
        # ì¸í”Œë£¨ì–¸ì„œ ì¡°íšŒ
        influencer = get_influencer_by_id(db, user_id, influencer_id)
        if not influencer:
            raise HTTPException(status_code=404, detail="ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
        from app.models.influencer import AIInfluencer
        db.query(AIInfluencer).filter(
            AIInfluencer.influencer_id == influencer_id,
            AIInfluencer.user_id == user_id
        ).update({
            "system_prompt": request.data.strip()
        })
        
        db.commit()
        
        logger.info(f"âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ: influencer_id={influencer_id}, type={request.type}")
        
        return {
            "message": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤",
            "influencer_id": influencer_id,
            "type": request.type,
            "system_prompt_saved": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")